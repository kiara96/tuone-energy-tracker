{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import RobertaForSpanCategorization\n",
    "from transformers import RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSpanCategorization.from_pretrained(\"C:/Users/Samsung/OneDrive/Desktop/github/tuone-energy-tracker/models/fine_tune_bert_output/\")\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets_and_predicted_tags(example: str, model, tokenizer, threshold=0):\n",
    "    \"\"\"\n",
    "    Get prediction of model on example, using tokenizer\n",
    "    Args:\n",
    "      - example (str): The input text\n",
    "      - model: The span categorizer\n",
    "      - tokenizer: The tokenizer\n",
    "      - threshold: The threshold to decide whether the token should belong to the label. Default to 0, which corresponds to probability 0.5.\n",
    "    Returns:\n",
    "      - List of (token, tags, offset) for each token.\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence to retrieve the tokens and offset mappings\n",
    "    raw_encoded_example = tokenizer(example, return_offsets_mapping=True)\n",
    "    encoded_example = tokenizer(example, return_tensors=\"pt\")\n",
    "    \n",
    "    # Call the model. The output LxK-tensor where L is the number of tokens, K is the number of classes\n",
    "    out = model(**encoded_example)[\"logits\"][0]\n",
    "    \n",
    "    # We assign to each token the classes whose logit is positive\n",
    "    predicted_tags = [[i for i, l in enumerate(logit) if l > threshold] for logit in out]\n",
    "    \n",
    "    return [{\"token\": token, \"tags\": tag, \"offset\": offset} for (token, tag, offset) \n",
    "            in zip(tokenizer.batch_decode(raw_encoded_example[\"input_ids\"]), \n",
    "                   predicted_tags, \n",
    "                   raw_encoded_example[\"offset_mapping\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>             - []\n",
      "G               - []\n",
      "SR              - []\n",
      " Capital        - []\n",
      " invests        - []\n",
      " in             - []\n",
      " NE             - []\n",
      "VS              - []\n",
      " &              - []\n",
      " building       - []\n",
      " battery        - []\n",
      " factory        - []\n",
      "</s>            - []\n"
     ]
    }
   ],
   "source": [
    "example = \"GSR Capital invests in NEVS & building battery factory\"\n",
    "for item in get_offsets_and_predicted_tags(example, model, tokenizer):\n",
    "    print(f\"\"\"{item[\"token\"]:15} - {item[\"tags\"]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "file_path = 'C:/Users/Samsung/OneDrive/Desktop/github/tuone-energy-tracker/data/tuone_labelling.jsonl'\n",
    "\n",
    "# Read the JSONL file into a list of dictionaries\n",
    "data = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract the \"text\" column into a list\n",
    "text_list = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = text_list[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "tag2id = {'ORG': 1, 'TECH': 2, 'LOC': 3, 'STATUS': 4, 'CAPACITY': 5, 'VALUE': 6, 'SUBSIDY': 7, 'JOBS': 8}\n",
    "id2tag = {v: k for k, v in tag2id.items()}\n",
    "\n",
    "# Label to ID mapping for \"IOB\" tagging scheme\n",
    "label2id = {\n",
    "    'O': 0,\n",
    "    **{f'B-{k}': 2*v - 1 for k, v in tag2id.items()},\n",
    "    **{f'I-{k}': 2*v for k, v in tag2id.items()}\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "def get_tagged_groups(example: str, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Get prediction of model on example, using tokenizer\n",
    "    Returns:\n",
    "    - List of spans under offset format {\"start\": ..., \"end\": ..., \"tag\": ...}, sorted by start, end then tag.\n",
    "    \"\"\"\n",
    "    offsets_and_tags = get_offsets_and_predicted_tags(example, model, tokenizer)\n",
    "    predicted_offsets = {l: [] for l in tag2id}\n",
    "    last_token_tags = []\n",
    "    for item in offsets_and_tags:\n",
    "        (start, end), tags = item[\"offset\"], item[\"tags\"]\n",
    "        \n",
    "        for label_id in tags:\n",
    "            label = id2label[label_id]\n",
    "            tag = label[2:] # \"I-PER\" => \"PER\"\n",
    "            if label.startswith(\"B-\"):\n",
    "                predicted_offsets[tag].append({\"start\": start, \"end\": end})\n",
    "            elif label.startswith(\"I-\"):\n",
    "                # If \"B-\" and \"I-\" both appear in the same tag, ignore as we already processed it\n",
    "                if label2id[f\"B-{tag}\"] in tags:\n",
    "                    continue\n",
    "                \n",
    "                if label_id not in last_token_tags and label2id[f\"B-{tag}\"] not in last_token_tags:\n",
    "                    predicted_offsets[tag].append({\"start\": start, \"end\": end})\n",
    "                else:\n",
    "                    predicted_offsets[tag][-1][\"end\"] = end\n",
    "        \n",
    "        last_token_tags = tags\n",
    "        \n",
    "    flatten_predicted_offsets = [{**v, \"tag\": k, \"text\": example[v[\"start\"]:v[\"end\"]]} \n",
    "                                 for k, v_list in predicted_offsets.items() for v in v_list if v[\"end\"] - v[\"start\"] >= 3]\n",
    "    flatten_predicted_offsets = sorted(flatten_predicted_offsets, \n",
    "                                       key = lambda row: (row[\"start\"], row[\"end\"], row[\"tag\"]))\n",
    "    return flatten_predicted_offsets\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Process each text in the list\n",
    "for text in text_list:\n",
    "    tagged_groups = get_tagged_groups(text, model, tokenizer)\n",
    "    results.append(tagged_groups)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2511"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the data from the JSONL file\n",
    "file_path = \"C:/Users/Samsung/OneDrive/Desktop/github/tuone-energy-tracker/data/tuone_labelling.jsonl\"\n",
    "max_length = 0\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line.strip())\n",
    "        text_length = len(data[\"text\"])\n",
    "        if text_length > max_length:\n",
    "            max_length = text_length\n",
    "\n",
    "max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
